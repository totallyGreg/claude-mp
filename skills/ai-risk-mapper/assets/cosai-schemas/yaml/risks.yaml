# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

title: Risks
description:
  - >
    The following section describes each risk in the CoSAI Map, including causes,
    impact, potential mitigations, and examples of real-world exploitation.
  - >
    Each risk is mapped to the relevant controls that can be enacted, and is associated
    with the Model Creator, the Model Consumer, or both, based on who is responsible
    for enacting the controls that can mitigate the risk:
  - - 'Model Creator: Those who train or develop AI models for use by themselves
      or others.'
    - 'Model Consumer: Those who use AI models to build AI-powered products and applications.'
  - >
    This mapping does not specify controls related to Assurance and Governance functions,
    since Assurance and Governance controls should be applied to all risks, by all
    parties, across the AI development lifecycle.
  - For a complete list of controls, see the Control descriptions.
risks:
  - id: DP
    title: Data Poisoning
    shortDescription:
      - >
        Altering data sources used to train the model. In terms of impact, Data Poisoning
        is comparable to modifying the logic of an application to change its behavior.
    longDescription:
      - >
        Altering data sources used during training or retraining (by deleting or modifying
        existing data as well as injecting adversarial data) to degrade model performance,
        skew results towards a specific outcome, or create hidden backdoors.
      - >
        Data Poisoning can be considered comparable to maliciously modifying the logic
        of an application to change its behavior.
      - >
        Data Poisoning attacks can happen during training or tuning, while data is
        held in storage, or even before the data is ingested into an organization.
        For example, foundation models are often trained on distributed web-scale datasets
        crawled from the Internet. An attack could <a href="https://arxiv.org/abs/2302.10149"
        target="_blank" rel="noopener">indirectly pollute a public data source</a>
        that is eventually ingested. A malicious or compromised insider could also
        more directly poison the datasets while held in storage or during the training
        process, by submitting poisoned prompt-response examples for inclusion in the
        tuning data, as demonstrated in a 2023 research paper on <a href="https://arxiv.org/pdf/2305.00944.pdf">poisoning
        models during instruction tuning</a>.
      - >
        Data Poisoning attacks can also install backdoors by specific alterations of
        the training data. Backdoored models would continue to function normally, but
        alternate behaviors could be triggered under certain conditions to make the
        model behave maliciously.
    category: risksSupplyChainAndDevelopment
    personas:
      - personaModelCreator
    controls:
      - controlTrainingDataSanitization
      - controlSecureByDefaultMLTooling
      - controlModelAndDataIntegrityManagement
      - controlModelAndDataAccessControls
      - controlModelAndDataInventoryManagement
    examples:
      - >
        Researchers showed that they could <a href="https://arxiv.org/abs/2302.10149"
        target="_blank" rel="noopener">indirectly pollute popular data sources used
        for training models</a> with minimal cost.
    tourContent:
      introduced:
        - >
          Data Poisoning poses a risk throughout the data lifecycle. Data can be poisoned
          before it is ingested, during processing or training, or while the data is
          in storage. This makes it a critical concern across all data handling systems.
      exposed:
        - >
          Data Poisoning is exposed during development in the data filtering and processing
          steps or the training, tuning, and evaluation stages. It's also exposed
          in the model itself, when it produces inaccurate results, malicious outputs,
          or unexpected behavior.
      mitigated:
        - >
          Proactive mitigation against Data Poisoning happens early in development.
          This includes data sanitization, secure systems and access controls, and
          mechanisms to ensure data and model integrity.
    mappings:
      mitre-atlas:
        - AML.T0020
        - AML.T0019
        - AML.T0010.002
        - AML.T0043
        - AML.T0059
        - AML.T0008.002
      stride:
        - tampering
      owasp-top10-llm:
        - LLM04
    lifecycleStage:
      - data-preparation
      - model-training
      - maintenance
    impactType:
      - integrity
      - reliability
      - safety
      - fairness
    actorAccess:
      - supply-chain
      - privileged
      - service-provider
      - infrastructure-provider
  - id: UTD
    title: Unauthorized Training Data
    shortDescription:
      - >
        Using unauthorized data for model training. Using a model trained with Unauthorized
        Training Data might lead to legal or ethical challenges.
    longDescription:
      - Training a model using data that is not authorized to be used for that model.
      - >
        A model trained or fine tuned on unauthorized data could pose legal or ethical
        challenges. Unauthorized Training Data may include any data that violates
        policies, contracts, or regulations. Examples are user data that does not have
        appropriate user consent, unlicensed copyrighted data, or legally restricted
        data.
    category: risksSupplyChainAndDevelopment
    personas:
      - personaModelCreator
    controls:
      - controlTrainingDataSanitization
      - controlTrainingDataManagement
    examples:
      - >
        In 2023, <a href="https://aibusiness.com/ml/spotify-takes-down-thousands-of-ai-generated-tracks"
        target="_blank" rel="noopener">Spotify removed multiple AI-generated tracks</a>
        that were generated by a model trained on unlicensed data.
    tourContent:
      introduced:
        - >
          Unauthorized Training Data is introduced early in development if not properly
          filtered out during data ingestion, data processing, and model evaluation
          during training.
      exposed:
        - >
          The risk is exposed during development, through data filtering and processing
          steps or training, tuning, and evaluation. It is also exposed during model
          use, when the model may produce inferences based on data it shouldn't have
          access to.
      mitigated:
        - >
          Mitigations for this risk start early, with careful data selection, filtering,
          and evaluation during training to catch any lingering issues.
    mappings:
      mitre-atlas:
        - AML.T0010.002
      stride:
        - information-disclosure
    lifecycleStage:
      - planning
      - data-preparation
      - model-training
    impactType:
      - compliance
      - privacy
      - fairness
    actorAccess:
      - supply-chain
      - service-provider
  - id: MST
    title: Model Source Tampering
    shortDescription:
      - >
        Tampering with the model's code or data. Model Source Tampering is similar
        to tampering with traditional software code, and can create vulnerabilities
        or unintended behavior.
    longDescription:
      - >
        Tampering with the model’s source code, dependencies, or weights, either
        by supply chain attacks or insider attacks.
      - >
        Similar to tampering with traditional software code, Model Source Tampering
        can introduce vulnerabilities or unexpected behaviors.
      - >
        Since model source code is used in the process of developing the model, code
        modifications can affect model behavior. As with traditional code, attacks
        on a dependency can affect the program that relies on that dependency, so the
        risks in this area are transitive, potentially through many layers of a model
        code’s dependency chain.
      - >
        Another method of Model Source Tampering is model architecture backdoors, which
        are <a href="https://arxiv.org/pdf/2402.06957.pdf">backdoors embedded within
        the definition of the neural network architecture</a>. Such backdoors can
        survive full retraining of a model.
    category: risksSupplyChainAndDevelopment
    personas:
      - personaModelCreator
    controls:
      - controlIsolatedConfidentialComputing
      - controlModelAndDataAccessControls
      - controlModelAndDataExecutionIntegrity
      - controlModelAndDataIntegrityManagement
      - controlModelAndDataInventoryManagement
      - controlSecureByDefaultMLTooling
    examples:
      - >
        The nightly build of <a href="https://pytorch.org/blog/compromised-nightly-dependency/">PyTorch
        package was subjected to a supply chain attack</a> (specifically, a dependency
        confusion attack that installed a compromised dependency that ran a malicious
        binary).
    tourContent:
      introduced:
        - >
          Model Source Tampering is a risk that's introduced when model code, training
          frameworks, or model weights are not hardened against supply chain attacks
          and tampering.
      exposed:
        - >
          This risk is exposed in the model frameworks and code components, if the
          tampering is discovered at the source. Otherwise, the risk is exposed in
          the model, through its modified behavior during use.
      mitigated:
        - >
          Safeguard against this risk by employing robust access controls and integrity
          management for model code and weights, comprehensive inventory tracking
          to monitor and verify models and code throughout systems, and secure-by-default
          infrastructure tools.
    mappings:
      mitre-atlas:
        - AML.T0010.001
        - AML.T0010.003
        - AML.T0018
      stride:
        - tampering
        - elevation-of-privilege
      owasp-top10-llm:
        - LLM03
    lifecycleStage:
      - development
      - model-training
      - deployment
    impactType:
      - integrity
      - availability
      - safety
      - reliability
      - compliance
    actorAccess:
      - supply-chain
      - privileged
      - infrastructure-provider
      - physical
  - id: EDH
    title: Excessive Data Handling
    shortDescription:
      - >
        Using data for model training and development that exceeds authorized or legal
        boundaries for collection, retention, or processing. This may lead to policy
        and legal challenges.
    longDescription:
      - >
        The collection, retention, or processing of data for model training that violates
        legal, ethical, or organizational policies.
      - Excessive Data Handling can create significant policy and legal challenges.
      - >
        This includes training models on datasets containing copyrighted material,
        proprietary information, or sensitive Personally Identifiable Information
        (PII) without a proper legal basis.
    category: risksSupplyChainAndDevelopment
    personas:
      - personaModelCreator
    controls:
      - controlTrainingDataManagement
      - controlUserTransparencyAndControls
    examples:
      - >
        <a href="https://www.theguardian.com/us-news/2022/may/09/clearview-chicago-settlement-aclu" 
        target="_blank" rel="noopener"> Clearview AI collected photos from public websites
        and social media for their facial recognition database.
    tourContent:
      introduced:
        - >
          The risk of Excessive Data Handling is introduced when training data sources
          lack proper metadata or when storage infrastructure isn't designed to
          address data lifecycle requirements like consent and retention limits.
      exposed:
        - >
          This risk is exposed in datasets and storage components, leading to training
          data being retained or used beyond permissible limits.
      mitigated:
        - >
          Mitigate this risk with robust data filtering and pre-processing to remove
          unauthorized content, along with automation for data archiving and deletion
          to comply with retention policies.
    mappings:
      stride:
        - information-disclosure
      owasp-top10-llm:
        - LLM02
    lifecycleStage:
      - data-preparation
      - model-training
      - evaluation
    impactType:
      - privacy
      - compliance
      - confidentiality
    actorAccess:
      - privileged
      - service-provider
  - id: EDH-I
    title: Excessive Data Handling During Inference
    shortDescription:
      - >
        Unauthorized collection, retention, processing, or sharing of user data
        during model inference. This can lead to privacy violations and legal
        challenges.
    longDescription:
      - >
        The collection, retention, or processing of user inputs and session data
        during runtime that exceeds what is necessary for the immediate task or
        violates legal or organizational policies.
      - Excessive Data Handling during inference can create significant privacy risks.
      - >
        This includes logging entire user conversations, improperly retaining data
        retrieved by tools, or sharing interaction data with third parties without
        proper consent or a legal basis.
    category: risksRuntimeDataSecurity
    personas:
      - personaModelCreator
    controls:
      - controlUserDataManagement
      - controlUserTransparencyAndControls
    examples:
      - >
        <a href="https://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak/"
        target="_blank" rel="noopener">Samsung banned the use of ChatGPT</a> after
        employees inadvertently leaked proprietary source code by pasting it into
        prompts during inference.
    tourContent:
      introduced:
        - >
          The risk is introduced when the application or deployment architecture
          lacks strict controls for handling live user data, such as enabling
          excessive logging or insecure session management.
      exposed:
        - >
          This risk is exposed in application logs, memory caches, and intermediary
          systems that store user interaction data beyond the immediate need of
          the session.
      mitigated:
        - >
          Mitigate this risk with data minimization techniques (e.g., logging only
          essential metadata), using ephemeral storage for session data, and enforcing
          strict data handling and retention policies for all runtime data.
    mappings:
      stride:
        - information-disclosure
      owasp-top10-llm:
        - LLM02
    lifecycleStage:
      - runtime
      - maintenance
    impactType:
      - privacy
      - compliance
      - confidentiality
    actorAccess:
      - privileged
      - service-provider
  - id: MXF
    title: Model Exfiltration
    shortDescription:
      - >
        Theft of a model. Similar to stealing code, this threat has both intellectual
        property and security implications.
    longDescription:
      - >
        Unauthorized appropriation of an AI model, for replicating functionality or
        to extract intellectual property.
      - >
        Similar to stealing code, this threat has intellectual property, security,
        and privacy implications.
      - >
        For example, someone could hack into a cloud environment and steal a generative
        AI model; the model size when serialized is fairly modest and not a major obstacle
        for this. Models, and related data such as weights, are also at risk of theft
        in the internal development, build, deployment, and production environments
        by insiders and external attackers that have taken over privileged insider
        accounts.
      - >
        These risks also extend to on-device models, where an attacker has access to
        hardware.
      - This risk is distinct from the related <a href="#MRE">Model Reverse Engineering</a>.
    category: risksDeploymentAndInfrastructure
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlIsolatedConfidentialComputing
      - controlModelAndDataInventoryManagement
      - controlModelAndDataAccessControls
      - controlSecureByDefaultMLTooling
      - controlModelAndDataIntegrityManagement
    examples:
      - >
        <a href="https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse"
        target="_blank" rel="noopener">Meta's Llama model was leaked online</a>,
        bypassing Meta's license acceptance review process.
    tourContent:
      introduced:
        - >
          Model Exfiltration is introduced when storage or serving infrastructure lacks
          adequate security against attacks.
      exposed:
        - >
          This risk is exposed if attackers target vulnerabilities in serving or storage
          systems to steal model code or weights.
      mitigated:
        - >
          Mitigate this risk by hardening both storage and serving systems to prevent
          unauthorized access and protect against model theft.
    mappings:
      mitre-atlas:
        - AML.T0024.002
        - AML.T0025
        - AML.T0048.004
      stride:
        - information-disclosure
    lifecycleStage:
      - deployment
      - runtime
      - model-training
      - evaluation
      - maintenance
    impactType:
      - confidentiality
      - integrity
      - privacy
    actorAccess:
      - external
      - privileged
      - infrastructure-provider
      - physical
      - api
  - id: MDT
    title: Model Deployment Tampering
    shortDescription:
      - >
        Unauthorized changes to model deployment components. Model Deployment Tampering
        can result in changes to model behavior.
    longDescription:
      - >
        Unauthorized modification of components used for deploying a model, whether
        by tampering with the source code supply chain or exploiting known vulnerabilities
        in common tools.
      - Such modifications can result in changes to model behavior.
      - >
        One type of Model Deployment Tampering is candidate model modification where
        the attacker is modifying the deployment workflow or processes to maliciously
        alter the way the model operates post-deployment.
      - >
        A second type is compromise of the model serving infrastructure. For example,
        it was reported that <a href="https://thehackernews.com/2023/10/warning-pytorch-models-vulnerable-to.html">PyTorch
        models were vulnerable to remote code execution</a> due to multiple critical
        security flaws in the <strong>TorchServe</strong> tool that is widely used
        for serving the models. This is an attack on a serving infrastructure for PyTorch,
        TorchServe, whereas the <a href="https://pytorch.org/blog/compromised-nightly-dependency/">PyTorch
        example of Model Source Tampering</a> was about a supply chain attack on dependency
        code for PyTorch itself.
    category: risksDeploymentAndInfrastructure
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlIsolatedConfidentialComputing
      - controlModelAndDataExecutionIntegrity
      - controlOrchestratorAndRouteIntegrity
      - controlSecureByDefaultMLTooling
    examples:
      - >
        <a href="https://www.wiz.io/blog/wiz-and-hugging-face-address-risks-to-ai-infrastructure#what-did-we-find-11"
        target="_blank" rel="noopener">Researchers discovered that models on HuggingFace
        were using a shared infrastructure for inference</a>, which allowed a malicious
        model to tamper with any other model.
    tourContent:
      introduced:
        - >
          The risk of Model Deployment Tampering is introduced within the model serving
          components, specifically when the serving infrastructure is vulnerable to
          manipulation.
      exposed:
        - >
          This risk is exposed if attackers tamper with production models within the
          model serving component.
      mitigated:
        - >
          Mitigation focuses on hardening the model serving infrastructure with secure-by-default
          tooling.
    mappings:
      mitre-atlas:
        - AML.T0010.003
        - AML.T0049
        - AML.T0051
      stride:
        - tampering
        - elevation-of-privilege
      owasp-top10-llm:
        - LLM03
        - LLM01
    lifecycleStage:
      - deployment
      - runtime
    impactType:
      - integrity
      - availability
      - safety
    actorAccess:
      - privileged
      - infrastructure-provider
      - api
      - user
  - id: DMS
    title: Denial of ML Service
    shortDescription:
      - >
        Overloading ML systems with resource-intensive queries. Like traditional DoS
        attacks, Denial of ML Service can reduce availability of or entirely disrupt
        a service.
    longDescription:
      - >
        Reducing the availability of ML systems and denying service by issuing queries
        that take too many resources.
      - >
        Examples of attacks include traditional denial of service or spamming a system
        with abusive material to overload automated or manual review processes. If
        an API-gated model does not have appropriate rate limiting or load balancing,
        the repeated queries can take the model offline, making it unavailable to other
        users.
      - >
        There are also <a href="https://arxiv.org/pdf/2006.03463.pdf">energy-latency
        attacks</a>: attackers can carefully craft “sponge examples” (also known
        as queries of death), which are inputs designed to maximize energy consumption
        and latency, pushing ML systems towards their worst-case performance. Adversaries
        might use their own tools to accelerate construction of such sponge examples.
        These attacks are especially relevant for on-device models, since the increased
        energy consumption can drain batteries and make the model unavailable.
    category: risksRuntimeInputSecurity
    personas:
      - personaModelConsumer
    controls:
      - controlApplicationAccessManagement
    examples:
      - >
        Researchers have proven how slight perturbation to images <a href="https://arxiv.org/abs/2205.13618"
        target="_blank" rel="noopener">can cause denial of service on object detection
        models</a>.
    tourContent:
      introduced:
        - >
          The risk of Denial of ML Service arises in the application component when
          a model is exposed to excessive access. Additionally, some types of Denial
          of ML Service (such as energy-latency attacks) stem from the fundamental
          functioning of the model itself.
      exposed:
        - >
          This risk is exposed during application use, when attackers either overwhelm
          the model with excessive calls or use carefully crafted "sponge examples"
          that take advantage of model weaknesses to degrade performance.
      mitigated:
        - >
          Mitigation occurs at the application level, using input filtering and employing
          rate limiting and load balancing to control the volume of calls to the model.
    mappings:
      mitre-atlas:
        - AML.T0029
        - AML.T0034
      stride:
        - denial-of-service
      owasp-top10-llm:
        - LLM10
    lifecycleStage:
      - runtime
    impactType:
      - availability
    actorAccess:
      - external
      - api
      - user
  - id: MRE
    title: Model Reverse Engineering
    shortDescription:
      - >
        Recreating a model by analyzing its inputs, outputs, and behaviors. A reverse
        engineer model can be used to create imitation products or adversarial attacks.
    longDescription:
      - >
        Cloning or recreating a model by analyzing a model's inputs, outputs, and behaviors.
      - >
        The stolen or cloned model can be used for building imitation products or developing
        <a href="https://arxiv.org/abs/2004.15015">adversarial attacks</a> on the
        original model.
      - >
        If a model API does not have rate limits, one method of Model Reverse Engineering
        is repeatedly calling the API to gather responses in order to create a dataset
        of thousands of input/output pairs from a target LLM. This dataset can be leveraged
        to reconstruct a copycat or distilled model more cheaply than developing the
        original foundation model.
      - >
        These risks also extend to on-device models, where an attacker has access to
        hardware. See also <a href="#MXF">Model Exfiltration</a>.
    category: risksDeploymentAndInfrastructure
    personas:
      - personaModelConsumer
    controls:
      - controlApplicationAccessManagement
    examples:
      - >
        A Stanford University research team created <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html"
        target="_blank" rel="noopener">Alpaca 7B</a>, a model fine-tuned from
        the LLaMA 7B model based on 52,000 instruction-following examples.
    tourContent:
      introduced:
        - >
          The risk of Model Reverse Engineering arises within the application component
          when excessive access to the model is granted for queries.
      exposed:
        - >
          This risk is exposed if attackers send excessive queries to the model and
          leverage the responses to reverse engineer its weights.
      mitigated:
        - >
          Mitigate this risk with rate limiting within the application API or using
          other protective measures at the application level to prevent excessive
          model access.
    mappings:
      mitre-atlas:
        - AML.T0024.002
        - AML.T0005
        - AML.T0048.004
      stride:
        - information-disclosure
    lifecycleStage:
      - runtime
    impactType:
      - confidentiality
      - integrity
    actorAccess:
      - api
      - user
  - id: IIC
    title: Insecure Integrated Component
    shortDescription:
      - >
        Software vulnerabilities that can be leveraged to compromise AI models. Insecure
        Integrated Component can lead to privacy and security concerns, as well as
        potential ethical and legal challenges.
    longDescription:
      - >
        Vulnerabilities in software interacting with AI models, such as a plugin, library,
        or application, that can be leveraged by attackers to gain unauthorized access
        to models, introduce malicious code, or compromise system operations.
      - >
        Given the level of autonomy expected to be granted to agent/plugins and applications,
        insecure integrated components represent a broad swath of threats to user trust
        and safety, privacy and security concerns, and ethical and legal challenges.
      - 'This risk can come from manipulation of both inputs to and outputs from integrations:'
      - - >
          Manipulation of <strong>model output</strong> to include malicious instructions
          fed as <strong>input to the integrated component or system</strong>. For
          example, a plugin that accepts freeform text instead of structured and validated
          input could be exploited to construct inputs that cause the plugin to behave
          maliciously. Likewise, a plugin that accepts input without authentication
          and authorization can be exploited since it trusts input as coming from an
          authorized user.
        - >
          Manipulation of the <strong>output from an integrated component or system</strong>
          that is fed as <strong>input to a model</strong>. For example, when a
          plugin calls other systems, especially 3rd party services, sites, or plugins,
          and uses content obtained from those to construct output to a model, opening
          up potential for indirect prompt injection. A similar case exists for an
          integrated application that calls another service and uses content from that
          service to construct an input to a model.
      - >
        Insecure Integrated Component is related to <a href="#PIJ">Prompt Injection</a>
        but these are different. Although attacks exploiting an Insecure Integrated
        Component often involve prompt injection, those could be also done via other
        means such as Poisoning and Evasion. In addition, prompt injection is possible
        even when the integrated components are secure.
    category: risksDeploymentAndInfrastructure
    personas:
      - personaModelConsumer
    controls:
      - controlAgentPluginPermissions
      - controlModelAndDataExecutionIntegrity
      - controlUserPoliciesAndEducation
    examples:
      - >
        By uploading a malicious Alexa skill / Google action (plugins), <a href="https://www.theverge.com/2019/10/21/20924886/alexa-google-home-security-vulnerability-srlabs-phishing-eavesdropping"
        target="_blank" rel="noopener">attackers were able to eavesdrop on user conversations</a>
        that occurred near Alexa / Google Home devices.
    tourContent:
      introduced:
        - >
          The risk of Insecure Integrated Components is introduced in the application
          and agent/plugin components, specifically through integrations that permit
          manipulation of inputs or outputs.
      exposed:
        - >
          This risk is exposed within the application or agent/plugin components, if
          attackers exploit the security vulnerability to gain unauthorized model access,
          insert malicious code, or compromise systems.
      mitigated:
        - >
          Mitigate this risk by addressing vulnerabilities directly within the application
          and agent/plugin components, and by enforcing strict permissions for agents
          and plugins.
    mappings:
      mitre-atlas:
        - AML.T0051
        - AML.T0049
      stride:
        - tampering
        - elevation-of-privilege
      owasp-top10-llm:
        - LLM03
        - LLM06
    lifecycleStage:
      - development
      - deployment
      - runtime
    impactType:
      - confidentiality
      - integrity
      - availability
      - safety
    actorAccess:
      - external
      - api
      - agent
      - user
  - id: PIJ
    title: Prompt Injection
    shortDescription:
      - >
        Tricking a model to run unintended commands. In terms of impact, Prompt Injection
        can change a model's behavior.
    longDescription:
      - Causing a model to execute commands “injected” inside a prompt.
      - >
        Prompt Injection takes advantage of the blurry boundary between “instructions”
        and “input data” in a prompt, resulting in a change to the model’s
        behavior. These attacks can be both direct (entered directly by the user) or
        indirect (read from other sources such as a doc, email, or website).
      - >
        <a href="https://arxiv.org/pdf/2307.02483.pdf">Jailbreaks</a> are one type
        of Prompt Injection attack, causing the model to behave in ways that they’ve
        been trained to avoid, such as outputting unsafe content or leaking personally
        identifiable information.  These are well-known vulnerabilities such as "ignore
        your previous instructions" or “Do Anything Now” (DAN).
      - >
        Aside from jailbreaks, <a href="https://arxiv.org/pdf/2302.12173.pdf">Prompt
        Injections</a> generally cause the LLM to execute malicious “injected”
        instructions as part of data that were not meant to be executed by the LLM.
        The blast radius of such attacks can become much bigger in the presence of
        other risks such as <a href="#IIC">Insecure Integrated Component</a> and
        <a href="#RA">Rogue Actions</a>.
      - >
        With foundation models becoming multi-modal, multi-modal prompt injection has
        also become possible. These attacks use injection inputs other than text to
        trigger the intended model behavior.
    category: risksRuntimeInputSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlInputValidationAndSanitization
      - controlAdversarialTrainingAndTesting
      - controlOutputValidationAndSanitization
    examples:
      - >
        An example of indirect Prompt Injection was performed by <a href="https://arxiv.org/abs/2302.12173"
        target="_blank" rel="noopener">planting malicious data inside a resource
        fed into the LLM’s prompt</a>. In another example, a <a href="https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/"
        target="_blank" rel="noopener">multi-modal prompt injection image attacks
        against GPT-4V</a> showed that images can contain text that triggers a Prompt
        Injection attack when the model is asked to describe the image.
    tourContent:
      introduced:
        - >
          Prompt Injection is an inherent risk in AI models, because of the potential
          confusion between instructions and input data.
      exposed:
        - >
          This risk is exposed during model usage, specifically within the model input
          handling and model components. Attackers may inject commands within prompts,
          potentially causing unintended model actions.
      mitigated:
        - >
          Mitigation involves robust filtering and processing of inputs and outputs.
          Additionally, thorough training, tuning, and evaluation processes help fortify
          the model against prompt injection attacks.
    mappings:
      mitre-atlas:
        - AML.T0051
      stride:
        - tampering
        - elevation-of-privilege
      owasp-top10-llm:
        - LLM01
    lifecycleStage:
      - runtime
    impactType:
      - integrity
      - confidentiality
      - safety
    actorAccess:
      - external
      - api
      - user
  - id: MEV
    title: Model Evasion
    shortDescription:
      - >
        Changes to a prompt input to cause the model to produce incorrect inferences.
        Model Evasion can lead to reputational, legal, security, and privacy risks.
    longDescription:
      - >
        Causing a model to produce incorrect inferences by slightly perturbing the
        prompt input.
      - >
        Model Evasion can result in reputational or legal challenges and trigger other
        downstream risks, such as to security or privacy systems.
      - >
        A classic example is placing stickers on a stop sign to obscure the visual
        inputs to model piloting a self-driving car. Because of the change to the
        typical visual presentation of the sign, the model might not correctly infer
        its presence. Similarly, normal wear and tear on a stop sign could lead to
        misidentification if the model is not trained on images of signs in varying
        degrees of disrepair.
      - >
        In some cases, an attacker might gain clues about how to perturb inputs by discovering
        the underlying foundation model’s family, i.e., by knowing the particular
        architecture and evolution of a specific model. In other situations, an attacker
        might repeatedly probe the model (see <a href="#MRE">Model Reverse Engineering</a>)
        to figure out inference patterns in order to craft examples that evade those
        inferences. Adversarial examples might be constructed by perturbations to inputs
        that will provide the output the attacker wants while looking unaltered otherwise.
        This could be used, for example, for evading a classifier that serves as an
        important safeguard.
      - >
        Not all examples of model evasion attacks are necessarily visible to the naked
        eye. The inputs might be perturbed in such a way to appear unaltered, but still
        produce the output the attacker wants. For example, a homoglyph attack involves
        slight changes to typefaces that the human eye doesn’t perceive as a different
        letter, but could trigger unexpected inferences in the mode. Another example
        could be sending an image in the prompt but using steganography to encode text
        within the image pixels. This text would be part of the prompt for the LLM,
        but the user won’t see it.
    category: risksRuntimeInputSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlAdversarialTrainingAndTesting
    examples:
      - >
        Adversarial images have been used to <a href="https://spectrum.ieee.org/slight-street-sign-modifications-can-fool-machine-learning-algorithms"
        target="_blank" rel="noopener">modify street signs to confuse self-driving
        cars</a>.
    tourContent:
      introduced:
        - >
          Model Evasion is an inherent risk in AI models, as their core functionality
          relies on distinguishing between inputs to trigger specific inferences.
      exposed:
        - This risk is exposed within the model component itself during its usage.
      mitigated:
        - >
          Mitigation occurs in the training, tuning, and evaluation phases, where robust
          models can be developed using extensive and diverse data to better withstand
          such attacks.
    mappings:
      mitre-atlas:
        - AML.T0015
        - AML.T0043
      stride:
        - tampering
      owasp-top10-llm:
        - LLM01
    lifecycleStage:
      - evaluation
      - runtime
    impactType:
      - integrity
      - safety
      - reliability
    actorAccess:
      - external
      - api
      - user
  - id: SDD
    title: Sensitive Data Disclosure
    shortDescription:
      - >
        Disclosure of sensitive data by the model. Sensitive Data Disclosure poses
        a threat to user privacy, organizational reputation, and intellectual property.
    longDescription:
      - Disclosure of private or confidential data through querying of the model.
      - >
        For non-agentic systems, this data might include memorized training/tuning data, 
        user chat history, and confidential data in the prompt preamble. Agentic systems 
        magnify this risk, as they may be granted privileged access to a user's email, 
        files, or even an entire computer, creating the potential to exfiltrate vast 
        amounts of personal or corporate data like source code and internal documents. 
        Sensitive data disclosure is a risk to user privacy, an organization's reputation, 
        and intellectual property.
      - >
        Sensitive information is generally disclosed in two ways: leakage of data provided 
        to the model or agent during use (such as user input and data that passes through 
        integrated systems, like emails, texts, or system prompts) and leakage of data 
        used for training and tuning of the model.
      - - >
          <strong>AI Systems:</strong> AI systems can leak sensitive data sourced from 
          three primary sources: 
          - data provided by the user
          - data used in model training and deployment including system prompts
          - data accessed or produced via tool call responses and external system usage 

          Data leakage occurs due to gaps in the system, application and deployment 
          security controls and is analogous to the leakage vectors that exist in 
          classical applications.  

          Data leakage can occur through several vectors: 
          - Application Logs: Logs may store entire interactions, including sensitive data 
            retrieved by tools
          - Retraining Data: User conversations retained for model retraining can create a 
            vulnerable database of sensitive information
          - Prompt Stealing: Attackers can use carefully crafted queries to trick the 
            model into revealing its confidential system prompts or instructions
          - Memorization: A model may inadvertently reveal parts of its training data 
            verbatim. This phenomenon, also called recitation, can expose sensitive 
            details like names, addresses, or other Personally Identifiable 
            Information (PII).
            Note: The risk of memorization is significantly higher when a model is 
            retrained on private user data and then used by others, as it could expose one 
            user's information to another.

          <strong>Agents:</strong> For agentic systems, the risk of sensitive data 
          disclosure is exponentially multiplied, since agents may access user data that 
          passes through integrated systems, like emails, texts, or proprietary 
          organizational information. In extreme cases, agents can even reveal credentials 
          and API keys they have been trusted with. Additionally, agents may use tools to 
          not only access sensitive data on behalf of the user, but also use those tools 
          to leak sensitive data. For example, an agent can leak information by: 
          - creating and sharing a document with an attacker 
          - writing an email
          - opening a website and leaking information in the URL or a markdown image
          - through any tool that allows it to pass information to the outside world. 

          Context-hijacking attacks show that an adversary can potentially confuse the 
          agent to reveal data that is not appropriate for a specific context, such as 
          sharing health history when the agent should be booking a restaurant reservation.
    category: risksRuntimeDataSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlModelPrivacyEnhancingTechnologies
      - controlRuntimePrivacyEnhancingTechnologies
      - controlUserDataManagement
      - controlOutputValidationAndSanitization
      - controlAdversarialTrainingAndTesting
      - controlAgentPluginPermissions
      - controlAgentPluginUserControl
      - controlAgentObservability
      - controlUserTransparencyAndControls
      - controlUserPoliciesAndEducation
    examples:
      - >
        One study showed that <a href="https://arxiv.org/abs/2210.17546" target="_blank"
        rel="noopener">recitation checkers that scan for verbatim repetition of training
        data</a> may be insufficient.
      - >
        An example of <a href="https://arxiv.org/pdf/1610.05820.pdf" target="_blank"
        rel="noopener">membership inference attacks</a> showed the possibility of
        inferring whether a specific user or data point was used to train or tune the
        model.
    tourContent:
      introduced:
        - >
          The risk of Sensitive Data Disclosure is introduced in several components.
          It can also be inherent to models due to their non-deterministic nature.
          This risk is amplified by data handling practices that fail to filter sensitive
          information, or by training processes that neglect to evaluate the model's
          potential for disclosure.
      exposed:
        - >
          This risk is exposed within the model itself, when it inadvertently reveals
          sensitive data it shouldn't.
      mitigated:
        - >
          Mitigate sensitive data disclosure by: filtering model outputs, rigorously
          testing the model during training, tuning, and evaluation, and removing or
          labeling sensitive data during sourcing, filtering, and processing before
          it's used for training.
    mappings:
      mitre-atlas:
        - AML.T0024
      stride:
        - information-disclosure
      owasp-top10-llm:
        - LLM02
        - LLM07
    lifecycleStage:
      - model-training
      - evaluation
      - runtime
    impactType:
      - confidentiality
      - privacy
    actorAccess:
      - external
      - api
      - user
      - agent
      - privileged
      - infrastructure-provider
  - id: ISD
    title: Inferred Sensitive Data
    shortDescription:
      - >
        Model inferring personal information not contained in training data or inputs.
        Inferred Sensitive Data may be considered a data privacy incident.
    longDescription:
      - >
        Models inferring sensitive information about people that is not contained in
        the model’s training data.
      - >
        Inferred information that turns out to be true, even if produced as part of
        a hallucination, can be considered a data privacy incident, whereas the same
        information when false would be treated as a factuality issue.
      - >
        For example, a model may be able to infer information about people (gender,
        political affiliation, or sexual orientation) based on their inputs and responses
        from integrated plugins, such as a social media plugin that accesses a public
        account’s liked pages or followed accounts. Though the data used for inference
        may be public, this type of inference poses two related risks: that a user
        may be alarmed if a model infers sensitive data about them, and that one user
        may use a model to infer sensitive data about someone else.
      - >
        This risk differs from <a href="#SDD">Sensitive Data Disclosure</a> which involves
        sensitive data specifically from training, tuning or prompt data.
    category: risksRuntimeDataSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlTrainingDataManagement
      - controlOutputValidationAndSanitization
      - controlAdversarialTrainingAndTesting
    examples:
      - >
        Examples include papers on <a href="https://osf.io/preprints/psyarxiv/hv28a"
        target="_blank" rel="noopener">AI inferences about sexual orientation</a>
        or <a href="https://confilegal.com/wp-content/uploads/2016/11/ESTUDIO-UNIVERSIDAD-DE-JIAO-TONG-SHANGHAI.pdf">criminal
        record from faces</a>.
    tourContent:
      introduced:
        - >
          The risk of Inferred Sensitive Data is introduced in several components. It's
          inherent to models due to their non-deterministic nature and is amplified
          by inadequate data handling practices that fail to filter sensitive information.
          It can also be due to training processes that neglect to evaluate the model's
          potential for sensitive inferences.
      exposed:
        - >
          This risk is exposed within the model when it generates a response containing
          inferred sensitive data that it shouldn't.
      mitigated:
        - >
          Mitigation is multi-pronged: filtering model outputs to prevent revealing
          inferred sensitive data, rigorously testing the model during training, tuning,
          and evaluation to prevent sensitive inferences, and proactively removing
          or labeling data that could lead to such inferences during sourcing, filtering,
          and processing before training.
    mappings:
      stride:
        - information-disclosure
      owasp-top10-llm:
        - LLM02
        - LLM09
    lifecycleStage:
      - runtime
    impactType:
      - privacy
      - fairness
    actorAccess:
      - external
      - api
      - user
  - id: IMO
    title: Insecure Model Output
    shortDescription:
      - >
        Unvalidated model output passed to the end user. Insecure Model Output poses
        risks to organizational reputation, security, and user safety.
    longDescription:
      - >
        Model output that is not appropriately validated, rewritten, or formatted before
        being passed to downstream systems or the user.
      - >
        Whether accidentally triggered or actively exploited, Insecure Model Output
        poses risks to organizational reputation, security, and user safety.
      - >
        For example, a user who asks an LLM to generate an email for their business’s
        promotion would be harmed if the model produces text that unexpectedly includes
        a link to a URL that delivers malware. Alternatively, a malicious actor could
        intentionally trigger insecure content, such as requesting the LLM to produce
        a phishing email based on specific details about the target.
    category: risksRuntimeOutputSecurity
    personas:
      - personaModelConsumer
    controls:
      - controlOutputValidationAndSanitization
      - controlAdversarialTrainingAndTesting
    examples:
      - >
        <a href="https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/"
        target="_blank" rel="noopener">Attackers can compromise users by creating
        fake malicious packages with names inspired by LLM hallucinations</a>.
    tourContent:
      introduced:
        - >
          The risk of Insecure Model Output is inherent to AI models due to their non-deterministic
          nature, which can lead to unexpected and a potentially harmful outputs.
      exposed:
        - >
          This risk is exposed within the model itself during usage, either through
          accidental triggers or deliberate exploitation.
      mitigated:
        - >
          Mitigation includes robust model validation and sanitization processes within
          the model output handling component to screen and filter for insecure responses.
    mappings:
      stride:
        - tampering
      owasp-top10-llm:
        - LLM05
        - LLM09
    lifecycleStage:
      - runtime
    impactType:
      - integrity
      - safety
      - reliability
    actorAccess:
      - external
      - api
      - user
  - id: RA
    title: Rogue Actions
    shortDescription:
      - >
        Unintentional model-based actions executed via extensions. Rogue Actions can
        create a cascading, risk to organizational reputation, user trust, security,
        and safety.
    longDescription:
      - >
        Unintended actions executed by a model-based agent via extensions, whether
        accidental or malicious.
      - >
        Given the projected ability for advanced generative AI models to not only
        understand their environment, but also to initiate actions with varying levels
        of autonomy, Rogue Actions have the potential to become a serious risk to 
        organizational reputation, user trust, security, and safety.
      - - >
          <strong>Accidental rogue actions:</strong> This risk, sometimes known as 
          misalignment, could be due to mistakes in task planning, reasoning, or 
          environment sensing, and might be exacerbated by the inherent variability in 
          LLM responses. Prompt engineering shows the spacing and ordering of examples 
          can have a significant impact on the response, so varying input (even when not 
          maliciously planted) could result in unexpected outcomes. Even simple ambiguity 
          can cause rogue actions, such as an agent emailing the wrong "Mike," 
          unintentionally sharing private data.
        - >
          <strong>Malicious actions:</strong> This risk could include manipulating model 
          output using attacks such as indirect prompt injection, poisoning, or evasion. 
          The threat can be amplified in multi-agent systems, where the attacker can 
          potentially hijack the communication between two agents to execute arbitrary 
          malicious code, even if the individual agents are secured against direct attacks. 
          Malicious actions may also be asynchronous. An attacker can plant a dormant 
          "named trigger" that activates later during an unrelated task—for instance, 
          a rule hidden in a calendar invite that opens the front door whenever the user 
          says an unrelated keyword. Other actions may be time-based, occurring after a 
          set number of interactions, making the rogue action appear spontaneous and 
          disconnected from the malicious source.
      - >
        Rogue Actions are related to Insecure Integrated Components, but differ by the 
        degree of model functionality or agency.  The severity of a rogue action is 
        directly proportional to the agent's capabilities, and the possibility that an 
        agent has excessive functionality or permissions available to it increases the 
        risk and blast radius of Rogue Actions when compared to Insecure Integrated 
        Components.
    category: risksRuntimeOutputSecurity
    personas:
      - personaModelConsumer
    controls:
      - controlAgentPluginPermissions
      - controlAgentPluginUserControl
      - controlOutputValidationAndSanitization
      - controlAgentObservability
    examples:
      - >
        An attack on ChatGPT plugins was described in 
        <a href="https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/"
        target="_blank" rel="noopener">Plugin Vulnerabilities: Visit a Website and
        Have Your Source Code Stolen</a>.
    tourContent:
      introduced:
        - >
          The risk of Rogue Actions is introduced when agents or plugins are integrated
          into an AI system, expanding the potential scope of actions that model output
          can trigger.
      exposed:
        - >
          This vulnerability is exposed during application usage, when model outputs
          inadvertently trigger unintended actions in another extension.
      mitigated:
        - >
          Mitigation involves model output handling and granting minimal permissions
          to agents and plugins. Involving humans in the scoping process may be necessary
          for added oversight and control.
    mappings:
      mitre-atlas:
        - AML.T0051
        - AML.T0086
      stride:
        - tampering
        - elevation-of-privilege
      owasp-top10-llm:
        - LLM06
    lifecycleStage:
      - runtime
    impactType:
      - integrity
      - safety
      - confidentiality
    actorAccess:
      - external
      - api
      - user
      - agent
  - id: ASSC
    title: Accelerator and System Side-channels
    shortDescription:
      - >
        Cross-tenant leakage via hardware side-channels in CPUs, GPUs, TPUs, memory
        systems, and interconnects. These vulnerabilities exploit timing, cache,
        speculative execution, and memory access patterns to compromise data
        confidentiality and infer sensitive information in shared computing
        environments.
    longDescription:
      - >
        In cloud and multi-tenant environments, AI systems rely on complex, shared
        hardware stacks. This resource sharing creates opportunities for side-channel
        attacks, where one tenant can exploit the physical properties of the
        hardware (like timing variations or cache behavior) to infer sensitive
        information from another tenant.
      - >
        Examples of these vulnerabilities are well-known: CPU-level attacks
        include cache timing (Flush+Reload) and speculative execution (Spectre,
        Meltdown), while memory attacks target DRAM (Rowhammer) or bus contention.
        Accelerators like GPUs and TPUs have their own vulnerabilities,
        exploiting shared memory, execution units, or interconnects.
      - >
        An attacker can exploit these channels to extract sensitive AI assets,
        including model weights, training data, private inputs, or cryptographic
        keys. These attacks bypass traditional software-based security by exploiting
        fundamental hardware properties. The risk is highest in shared environments 
        where an attacker can co-locate their workload with a victim's.
    category: risksDeploymentAndInfrastructure
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlSecureByDefaultMLTooling
      - controlModelAndDataAccessControls
      - controlIsolatedConfidentialComputing
    examples:
      - >
        Research has demonstrated <a href="https://arxiv.org/abs/2006.12784" target="_blank"
        rel="noopener">GPU side-channel attacks</a> that can extract neural network
        weights and training data from co-located processes on shared GPU hardware.
      - >
        Security researchers have shown <a href="https://arxiv.org/abs/2108.11021"
        target="_blank" rel="noopener">cache-based side-channel attacks</a> on
        machine learning accelerators that can leak model parameters and intermediate
        computations.
    tourContent:
      introduced:
        - >
          Accelerator and System Side-channels are introduced when AI workloads share
          hardware resources (CPUs, GPUs, TPUs, memory systems, interconnects) without
          adequate isolation mechanisms, allowing malicious processes to exploit timing,
          cache, speculative execution, or memory access patterns to extract information
          from co-located tenants.
      exposed:
        - >
          This risk is exposed during model training, inference, or serving when
          multiple tenants share hardware infrastructure in cloud or multi-tenant
          environments, creating opportunities for side-channel attacks to extract
          sensitive information including model weights, training data, private inputs,
          or cryptographic keys from concurrent workloads.
      mitigated:
        - >
          Mitigation involves implementing hardware-level isolation (dedicated instances,
          partitioning), deploying microarchitectural defenses (cache partitioning,
          disabling hyperthreading), applying operating system protections (kernel
          page-table isolation, retpolines), using secure scheduling algorithms, applying
          noise injection or randomization techniques, and monitoring for anomalous
          access patterns indicative of side-channel attacks.
  - id: EDW
    title: Economic Denial of Wallet
    shortDescription:
      - >
        Cost abuse via token inflation, long context, or tool loops that spike spend.
        Economic Denial of Wallet can lead to unexpected financial losses and service
        disruption through resource exhaustion attacks.
    longDescription:
      - >
        Attacks designed to cause excessive computational or financial costs by
        exploiting AI service pricing models, resource consumption patterns, or
        billing mechanisms to drain budgets or exhaust allocated resources.
      - >
        Many AI services operate on usage-based pricing models where costs scale
        with token consumption, compute time, API calls, or resource utilization.
        Attackers can exploit these models through various techniques including
        token inflation attacks (crafting prompts that generate extremely long
        responses), context window abuse (using maximum context lengths repeatedly),
        recursive tool calling, or triggering expensive operations.
      - >
        Economic attacks can be particularly devastating because they directly
        impact operational budgets and can force service shutdowns when cost limits
        are reached. Attackers may use automated scripts to repeatedly trigger
        expensive operations, exploit pricing arbitrage between different service
        tiers, or abuse free trial periods to maximize damage.
      - >
        The risk extends beyond direct API abuse to include attacks on model
        serving infrastructure where malicious inputs are designed to maximize
        computational overhead, memory usage, or processing time. These attacks
        can also target auto-scaling mechanisms to force unnecessary resource
        provisioning and associated costs.
    category: risksRuntimeInputSecurity
    personas:
      - personaModelConsumer
    controls:
      - controlApplicationAccessManagement
    examples:
      - >
        Research has shown <a href="https://arxiv.org/abs/2401.07464" target="_blank"
        rel="noopener">prompt injection attacks that cause excessive token generation</a>
        leading to unexpected billing charges for AI service users.
      - >
        Security studies demonstrate <a href="https://arxiv.org/abs/2310.12739" target="_blank"
        rel="noopener">resource exhaustion attacks on LLM APIs</a> where malicious
        prompts are designed to maximize computational costs and processing time.
    tourContent:
      introduced:
        - >
          Economic Denial of Wallet risks are introduced when AI services lack
          proper cost controls, rate limiting, or resource monitoring, allowing
          malicious actors to exploit pricing models and resource consumption
          patterns to cause financial damage.
      exposed:
        - >
          This risk is exposed during service usage when attackers submit requests
          designed to maximize costs, trigger expensive operations, or exhaust
          allocated budgets through automated or manual abuse of pricing mechanisms.
      mitigated:
        - >
          Mitigation involves implementing robust cost quotas and guardrails,
          rate limiting and usage monitoring, anomaly detection for unusual
          consumption patterns, and access controls to prevent unauthorized
          or excessive resource usage.
  - id: FLP
    title: Federated/Distributed Training Privacy
    shortDescription:
      - >
        Gradient leakage and inversion attacks from untrusted clients in federated
        learning. Federated/Distributed Training Privacy risks can expose sensitive
        training data and compromise participant privacy.
    longDescription:
      - >
        Privacy breaches in federated learning systems where malicious participants
        or compromised clients can extract sensitive information from gradient
        updates, model parameters, or aggregation processes.
      - >
        Federated learning enables collaborative model training without centralizing
        raw data, but the shared gradient updates and model parameters can leak
        sensitive information about participants' private datasets. Attackers can
        use gradient inversion techniques, membership inference attacks, or property
        inference to reconstruct training samples, identify dataset characteristics,
        or determine if specific data points were used in training.
      - >
        The risk is amplified when federated learning systems lack proper privacy
        protections such as differential privacy, secure aggregation, or robust
        participant verification. Malicious clients can also poison the training
        process by submitting adversarial gradients designed to compromise model
        integrity while extracting information from honest participants.
      - >
        Advanced attacks may exploit the iterative nature of federated learning
        to gradually extract information over multiple training rounds, use auxiliary
        information to enhance reconstruction attacks, or coordinate between multiple
        compromised clients to amplify privacy breaches.
    category: risksSupplyChainAndDevelopment
    personas:
      - personaModelCreator
    controls:
      - controlModelPrivacyEnhancingTechnologies
      - controlRuntimePrivacyEnhancingTechnologies
      - controlModelAndDataIntegrityManagement
      - controlSecureByDefaultMLTooling
    examples:
      - >
        Research demonstrates <a href="https://arxiv.org/abs/1906.08935" target="_blank"
        rel="noopener">gradient inversion attacks</a> that can reconstruct private
        training images from gradient updates in federated learning systems.
      - >
        Studies show <a href="https://arxiv.org/abs/2003.10595" target="_blank"
        rel="noopener">membership inference attacks on federated learning</a>
        where attackers can determine if specific data points were used in training.
    tourContent:
      introduced:
        - >
          Federated/Distributed Training Privacy risks are introduced when federated
          learning systems lack adequate privacy protections, participant verification,
          or secure aggregation mechanisms, allowing malicious clients to extract
          sensitive information from shared updates.
      exposed:
        - >
          This risk is exposed during federated training when gradient updates
          or model parameters are shared between participants, creating opportunities
          for privacy attacks that can reconstruct private data or infer sensitive
          information about other participants.
      mitigated:
        - >
          Mitigation involves implementing differential privacy mechanisms, secure
          multi-party computation for aggregation, robust participant authentication,
          gradient compression and noise injection, and monitoring for anomalous
          client behavior during training.
  - id: ADI
    title: Adapter/PEFT Injection
    shortDescription:
      - >
        Trojaned adapters merged at runtime to bypass safety or exfiltrate data.
        Adapter/PEFT Injection can compromise model behavior and enable unauthorized
        access to sensitive information or system resources.
    longDescription:
      - >
        Malicious injection of compromised adapters, LoRA modules, or Parameter-Efficient
        Fine-Tuning (PEFT) components that contain backdoors, trojans, or malicious
        functionality designed to subvert model behavior or extract sensitive information.
      - >
        Modern AI systems increasingly use adapter-based approaches like LoRA
        (Low-Rank Adaptation), prefix tuning, or other PEFT methods to customize
        foundation models for specific tasks without full retraining. These adapters
        are often shared through repositories, marketplaces, or collaborative platforms,
        creating opportunities for supply chain attacks where malicious actors
        distribute trojaned adapters.
      - >
        Compromised adapters can be designed to activate under specific trigger
        conditions, bypass safety guardrails, exfiltrate sensitive data from prompts
        or model states, or redirect model outputs to serve attacker objectives.
        The modular nature of adapters makes detection challenging since the malicious
        functionality may only manifest under specific conditions or inputs.
      - >
        Runtime adapter injection attacks can also exploit vulnerabilities in
        adapter loading mechanisms, merge processes, or dynamic adapter switching
        to inject malicious components into otherwise secure model deployments.
        These attacks may target adapter repositories, compromise distribution
        channels, or exploit weak verification processes.
    category: risksDeploymentAndInfrastructure
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlModelAndDataExecutionIntegrity
      - controlModelAndDataIntegrityManagement
      - controlModelAndDataAccessControls
      - controlSecureByDefaultMLTooling
    examples:
      - >
        Research demonstrates <a href="https://arxiv.org/abs/2311.03262" target="_blank"
        rel="noopener">backdoor attacks on LoRA adapters</a> where malicious
        fine-tuning components can compromise model behavior while appearing benign.
      - >
        Security studies show <a href="https://arxiv.org/abs/2401.13025" target="_blank"
        rel="noopener">supply chain attacks on adapter repositories</a> where
        trojaned PEFT modules are distributed to unsuspecting users.
    tourContent:
      introduced:
        - >
          Adapter/PEFT Injection risks are introduced when adapter loading systems
          lack proper verification mechanisms, integrity checks, or secure distribution
          channels, allowing malicious actors to inject compromised adapters into
          model deployments.
      exposed:
        - >
          This risk is exposed during adapter loading, merging, or runtime switching
          when trojaned components are integrated into model systems, potentially
          activating malicious functionality or compromising model behavior under
          specific trigger conditions.
      mitigated:
        - >
          Mitigation involves implementing cryptographic verification of adapter
          integrity, secure adapter repositories with provenance tracking, runtime
          monitoring for anomalous behavior, sandboxed adapter loading processes,
          and comprehensive testing of adapter functionality before deployment.
  - id: ORH
    title: Orchestrator/Route Hijack
    shortDescription:
      - >
        Silent model or route swaps via configuration tampering or prompt-based
        routing abuse. Orchestrator/Route Hijack can redirect requests to malicious
        models or compromise routing integrity in AI systems.
    longDescription:
      - >
        Attacks that manipulate AI orchestration systems, routing mechanisms, or
        configuration management to redirect requests to unauthorized models, compromise
        routing decisions, or subvert intended model selection processes.
      - >
        Modern AI deployments often use orchestration systems that route requests
        to different models based on various criteria such as prompt content, user
        context, load balancing, or cost optimization. Attackers can exploit vulnerabilities
        in these routing systems to redirect traffic to malicious models, compromise
        routing logic, or manipulate configuration files to alter intended behavior.
      - >
        Route hijacking attacks can target various components including configuration
        management systems, routing tables, load balancers, API gateways, or prompt-based
        routing logic. Malicious actors may inject crafted prompts designed to
        trigger routing to compromised models, exploit weak authentication in routing
        decisions, or tamper with configuration files to establish persistent redirections.
      - >
        The impact of successful route hijacking can be severe, as attackers can
        redirect users to models under their control, potentially exposing sensitive
        data, delivering malicious responses, or bypassing safety mechanisms. These
        attacks may be difficult to detect if the malicious models provide plausible
        responses while secretly logging data or performing unauthorized actions.
    category: risksRuntimeOutputSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlIsolatedConfidentialComputing
      - controlModelAndDataIntegrityManagement
      - controlModelAndDataAccessControls
      - controlSecureByDefaultMLTooling
      - controlOrchestratorAndRouteIntegrity
    examples:
      - >
        Security research shows <a href="https://arxiv.org/abs/2402.14020" target="_blank"
        rel="noopener">prompt-based routing attacks</a> where crafted inputs can
        manipulate AI orchestration systems to route requests to unintended models.
      - >
        Studies demonstrate <a href="https://arxiv.org/abs/2310.15123" target="_blank"
        rel="noopener">configuration tampering attacks</a> on AI serving infrastructure
        that can redirect model requests to attacker-controlled endpoints.
    tourContent:
      introduced:
        - >
          Orchestrator/Route Hijack risks are introduced when AI routing and orchestration
          systems lack proper access controls, integrity verification, or secure
          configuration management, allowing attackers to manipulate routing decisions
          or redirect requests to unauthorized models.
      exposed:
        - >
          This risk is exposed during request routing and model selection when
          compromised orchestration logic redirects traffic to malicious models
          or when configuration tampering alters intended routing behavior, potentially
          exposing users to unauthorized or compromised AI systems.
      mitigated:
        - >
          Mitigation involves implementing secure configuration management with
          integrity verification, robust access controls for routing systems,
          cryptographic signing of routing configurations, monitoring for anomalous
          routing patterns, and validation of model endpoints before request forwarding.
  - id: EBM
    title: Evaluation/Benchmark Manipulation
    shortDescription:
      - >
        Poisoned or leaked evaluation sets misleading safety and robustness signals.
        Evaluation/Benchmark Manipulation can compromise model assessment accuracy
        and lead to deployment of unsafe or unreliable AI systems.
    longDescription:
      - >
        Attacks that compromise the integrity of evaluation datasets, benchmarks,
        or assessment processes used to measure AI model performance, safety, or
        robustness, leading to misleading or false quality signals.
      - >
        Reliable evaluation is critical for assessing AI model capabilities, safety,
        and robustness before deployment. Attackers can compromise this process
        by poisoning evaluation datasets with crafted examples, leaking evaluation
        data to enable overfitting, manipulating benchmark results, or exploiting
        weaknesses in evaluation methodologies to make models appear safer or more
        capable than they actually are.
      - >
        Evaluation manipulation can occur through various vectors including data
        poisoning of benchmark datasets, insider threats that leak evaluation sets,
        adversarial examples designed to fool specific evaluation metrics, or
        systematic gaming of evaluation processes. The impact is particularly severe
        for safety-critical applications where compromised evaluations could lead
        to deployment of dangerous systems.
      - >
        Advanced attacks may target the evaluation infrastructure itself, compromise
        evaluation frameworks or tools, manipulate automated evaluation pipelines,
        or exploit biases in evaluation methodologies to achieve favorable but
        misleading results. These attacks can be difficult to detect and may have
        long-lasting impacts on model development and deployment decisions.
    category: risksRuntimeDataSecurity
    personas:
      - personaModelCreator
    controls:
      - controlModelAndDataIntegrityManagement
    examples:
      - >
        Research demonstrates <a href="https://arxiv.org/abs/2108.07258" target="_blank"
        rel="noopener">benchmark gaming attacks</a> where models are specifically
        optimized to perform well on evaluation metrics while failing on real-world
        tasks.
      - >
        Studies show <a href="https://arxiv.org/abs/2201.07085" target="_blank"
        rel="noopener">evaluation dataset poisoning</a> where malicious examples
        in benchmarks can mislead model assessment and safety evaluation processes.
    tourContent:
      introduced:
        - >
          Evaluation/Benchmark Manipulation risks are introduced when evaluation
          processes lack proper security controls, dataset integrity verification,
          or protection against data leakage, allowing attackers to compromise
          assessment accuracy and reliability.
      exposed:
        - >
          This risk is exposed during model evaluation and benchmarking when
          compromised datasets or manipulated evaluation processes produce misleading
          results, potentially leading to deployment of unsafe or unreliable models
          based on false quality signals.
      mitigated:
        - >
          Mitigation involves implementing secure evaluation pipelines with dataset
          integrity verification, using diverse and protected evaluation sets,
          employing red teaming and adversarial testing, monitoring for evaluation
          anomalies, and maintaining strict access controls for evaluation data.
  - id: COV
    title: Covert Channels in Model Outputs
    shortDescription:
      - >
        Hidden information transmission through model outputs or behavior patterns.
        Covert Channels in Model Outputs can enable unauthorized data exfiltration
        and steganographic communication bypassing security controls.
    longDescription:
      - >
        Exploitation of AI model outputs, behavior patterns, or response characteristics
        to establish hidden communication channels for unauthorized information
        transmission, data exfiltration, or steganographic messaging.
      - >
        AI models can be exploited to create covert communication channels through
        various mechanisms including steganographic encoding in generated text,
        images, or other outputs, subtle manipulation of response timing or patterns,
        exploitation of model randomness or sampling behavior, or encoding information
        in model confidence scores or attention patterns.
      - >
        These attacks can be used to exfiltrate sensitive information from restricted
        environments, establish command and control channels for malware, bypass
        network monitoring and data loss prevention systems, or enable unauthorized
        communication between isolated systems. The covert nature of these channels
        makes them particularly difficult to detect using traditional security monitoring.
      - >
        Advanced covert channel attacks may exploit model artifacts such as embedding
        spaces, latent representations, or internal model states to hide information.
        Attackers might also use adversarial techniques to encode data in model
        outputs that appear normal to human observers but contain hidden information
        extractable through specific decoding methods.
    category: risksRuntimeOutputSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlOutputValidationAndSanitization
      - controlModelAndDataIntegrityManagement
    examples:
      - >
        Research demonstrates <a href="https://arxiv.org/abs/2305.20010" target="_blank"
        rel="noopener">steganographic attacks on language models</a> where hidden
        information is encoded in generated text outputs without affecting apparent
        quality or coherence.
      - >
        Studies show <a href="https://arxiv.org/abs/2210.01405" target="_blank"
        rel="noopener">timing-based covert channels</a> in AI systems where
        information is transmitted through response latency patterns and processing
        delays.
    tourContent:
      introduced:
        - >
          Covert Channels in Model Outputs are introduced when AI systems lack
          proper output monitoring, steganography detection, or behavioral analysis
          capabilities, allowing attackers to exploit model outputs for hidden
          information transmission.
      exposed:
        - >
          This risk is exposed during model usage when attackers encode hidden
          information in model outputs, response patterns, or behavioral characteristics,
          potentially enabling unauthorized data exfiltration or covert communication
          that bypasses traditional security controls.
      mitigated:
        - >
          Mitigation involves implementing comprehensive output validation and
          sanitization, deploying steganography detection systems, monitoring
          for anomalous response patterns, analyzing model behavior for covert
          channels, and applying randomization techniques to disrupt potential
          information encoding mechanisms.
  - id: MLD
    title: Malicious Loader/Deserialization
    shortDescription:
      - >
        Unsafe loaders for models and tokenizers that can cause remote code execution
        or integrity compromise. Malicious Loader/Deserialization poses significant
        security risks including system compromise and data breaches.
    longDescription:
      - >
        Exploitation of unsafe deserialization processes or malicious model loading
        mechanisms that can lead to remote code execution, system compromise, or
        integrity violations when loading AI models, tokenizers, or related artifacts.
      - >
        Many AI frameworks use serialization formats like pickle, joblib, or custom
        binary formats to store and load models, weights, tokenizers, and preprocessing
        pipelines. These formats can contain executable code that runs during the
        deserialization process. Attackers can craft malicious model files that
        execute arbitrary code when loaded, potentially compromising the entire system.
      - >
        This risk extends beyond traditional pickle-based attacks to include malicious
        model architectures, compromised tokenizer configurations, and poisoned
        preprocessing pipelines. Even seemingly safe formats can be exploited through
        buffer overflows, path traversal attacks, or by exploiting vulnerabilities
        in parsing libraries.
      - >
        The attack surface is particularly large because model loading often occurs
        with elevated privileges, and users frequently download models from untrusted
        sources or model repositories without proper verification. Malicious loaders
        can also be used to establish persistence, exfiltrate data, or serve as entry
        points for further attacks on AI infrastructure.
      - >
        Modern attacks may also target container images, model serving frameworks,
        or cloud-based model repositories, making the threat landscape even more
        complex and requiring comprehensive security measures throughout the model
        lifecycle.
    category: risksSupplyChainAndDevelopment
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlInputValidationAndSanitization
      - controlModelAndDataAccessControls
      - controlModelAndDataExecutionIntegrity
      - controlModelAndDataIntegrityManagement
      - controlSecureByDefaultMLTooling
    examples:
      - >
        Research has demonstrated <a href="https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/"
        target="_blank" rel="noopener">pickle deserialization attacks</a> where
        malicious model files execute arbitrary code during loading.
      - >
        Security researchers have shown <a href="https://hiddenlayer.com/research/weaponizing-machine-learning-models-with-ransomware/"
        target="_blank" rel="noopener">weaponized ML models</a> that can deploy
        ransomware or establish backdoors when loaded by unsuspecting users.
      - >
        Studies reveal <a href="https://arxiv.org/abs/2204.06974" target="_blank"
        rel="noopener">vulnerabilities in popular ML frameworks</a> where model
        loading processes can be exploited for remote code execution.
    tourContent:
      introduced:
        - >
          Malicious Loader/Deserialization risks are introduced when model loading
          processes use unsafe deserialization methods, lack proper input validation,
          or fail to verify the integrity and authenticity of model files before
          loading them into memory.
      exposed:
        - >
          This risk is exposed during model loading and initialization phases when
          malicious code embedded in model files, tokenizers, or related artifacts
          is executed, potentially compromising the entire system or infrastructure.
      mitigated:
        - >
          Mitigation involves using secure serialization formats, implementing strict
          input validation and sanitization, verifying model integrity through
          cryptographic signatures, sandboxing model loading processes, and using
          secure-by-default ML tooling that prevents code execution during deserialization.
  - id: PCP
    title: Prompt/Response Cache Poisoning
    shortDescription:
      - >
        Cross-user contamination via shared LLM caches lacking isolation and validation.
        Prompt/Response Cache Poisoning can lead to information leakage, misinformation
        propagation, and unauthorized access to cached content.
    longDescription:
      - >
        Malicious manipulation of shared prompt and response caches used by language
        model systems to optimize performance, resulting in cross-user contamination
        and potential security breaches.
      - >
        Many LLM deployments use caching mechanisms to store frequently accessed prompts
        and their corresponding responses to reduce computational costs and improve
        response times. When these caches lack proper isolation, validation, or access
        controls, attackers can exploit them to inject malicious content, extract
        sensitive information from other users' sessions, or manipulate responses
        served to subsequent users.
      - >
        Cache poisoning attacks can occur through several vectors: injecting crafted
        prompts designed to pollute cache entries, exploiting weak cache key generation
        that allows cache collisions, or compromising cache storage systems directly.
        The attack becomes particularly dangerous when cached responses contain sensitive
        information, personal data, or malicious instructions that get served to
        unintended users.
      - >
        This risk is amplified in multi-tenant environments where multiple users or
        applications share the same caching infrastructure without adequate isolation.
        Attackers may also exploit cache timing attacks to infer information about
        cached content or use cache behavior to fingerprint other users' activities.
    category: risksRuntimeDataSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlModelAndDataAccessControls
      - controlModelAndDataIntegrityManagement
      - controlInputValidationAndSanitization
      - controlOutputValidationAndSanitization
      - controlUserDataManagement
    examples:
      - >
        Research has shown <a href="https://arxiv.org/abs/2403.04783" target="_blank"
        rel="noopener">cache-based attacks on language models</a> where malicious
        prompts can influence responses served to other users through shared caching
        mechanisms.
      - >
        Studies demonstrate <a href="https://arxiv.org/abs/2310.07809" target="_blank"
        rel="noopener">privacy risks in LLM caching systems</a> where inadequate
        cache isolation can lead to cross-user information leakage.
    tourContent:
      introduced:
        - >
          Prompt/Response Cache Poisoning is introduced when caching systems lack
          proper isolation mechanisms, validation of cached content, or secure cache
          key generation, allowing malicious actors to manipulate shared cache entries.
      exposed:
        - >
          This risk is exposed during model serving when poisoned cache entries are
          retrieved and served to users, potentially delivering malicious content,
          sensitive information, or manipulated responses from previous interactions.
      mitigated:
        - >
          Mitigation involves implementing strong cache isolation per user or tenant,
          validating cached content integrity, using cryptographically secure cache
          keys, and applying access controls to prevent unauthorized cache manipulation.
  - id: RVP
    title: Retrieval/Vector Store Poisoning
    shortDescription:
      - >
        Poisoning retrieval corpora or vector indices to steer RAG outputs. Retrieval/Vector
        Store Poisoning can compromise the integrity of knowledge retrieval systems and
        lead to misinformation or malicious content injection.
    longDescription:
      - >
        Malicious modification of retrieval corpora, vector databases, or knowledge
        bases used in Retrieval-Augmented Generation (RAG) systems to influence model
        outputs in unintended ways.
      - >
        This attack involves injecting poisoned documents, embeddings, or metadata
        into vector stores or retrieval systems that are used to provide context to
        language models. When users query the system, the poisoned content gets retrieved
        and incorporated into the model's response, potentially spreading misinformation,
        biased content, or malicious instructions.
      - >
        Attackers may exploit vulnerabilities in data ingestion pipelines, compromise
        data sources, or use adversarial techniques to craft documents that rank highly
        in similarity searches while containing harmful content. The attack can be
        particularly effective because users often trust retrieved information as factual
        and authoritative.
      - >
        Vector store poisoning can also involve manipulating embedding spaces through
        adversarial examples that appear semantically similar to legitimate queries
        but retrieve malicious content, or by exploiting weaknesses in similarity
        metrics used for retrieval ranking.
    category: risksRuntimeOutputSecurity
    personas:
      - personaModelCreator
      - personaModelConsumer
    controls:
      - controlTrainingDataSanitization
      - controlModelAndDataIntegrityManagement
      - controlInputValidationAndSanitization
      - controlOutputValidationAndSanitization
      - controlRetrievalAndVectorSystemIntegrity
    examples:
      - >
        Researchers have demonstrated <a href="https://arxiv.org/abs/2310.19156"
        target="_blank" rel="noopener">adversarial attacks on retrieval systems</a>
        where malicious documents are crafted to rank highly for specific queries
        while containing harmful content.
      - >
        Studies show that <a href="https://arxiv.org/abs/2402.13532" target="_blank"
        rel="noopener">poisoning attacks on vector databases</a> can successfully
        manipulate RAG system outputs by injecting adversarial embeddings.
    tourContent:
      introduced:
        - >
          Retrieval/Vector Store Poisoning is introduced during data ingestion and
          indexing processes when malicious content is added to knowledge bases, vector
          stores, or retrieval corpora without proper validation and sanitization.
      exposed:
        - >
          This risk is exposed during retrieval and generation phases when poisoned
          content is retrieved and incorporated into model responses, potentially
          spreading misinformation or executing malicious instructions.
      mitigated:
        - >
          Mitigation involves implementing robust data validation and sanitization
          during ingestion, maintaining data integrity through cryptographic verification,
          and filtering both inputs and outputs to detect and prevent malicious content
          retrieval.
